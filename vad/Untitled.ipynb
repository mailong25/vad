{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, json, argparse, glob\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import librosa as lr\n",
    "from tqdm import tqdm\n",
    "import ntpath\n",
    "import librosa\n",
    "\n",
    "def audio_from_file(path, sr=None, ext=''):\n",
    "    return lr.load('{}{}'.format(path, ext), sr=sr, mono=True, offset=0.0, duration=None, dtype=np.float32, res_type='kaiser_best')                \n",
    "\n",
    "def audio_to_file(path, x, sr):    \n",
    "    lr.output.write_wav(path, x.reshape(-1), sr, norm=False)   \n",
    "\n",
    "import soundfile as sf\n",
    "def convert_to_16k(in_path,out_path):\n",
    "    y, s = librosa.load(in_path, sr=16000)\n",
    "    y_16k = librosa.resample(y, s, 48000)\n",
    "    sf.write(out_path, y_16k, 48000, format='WAV', subtype='PCM_16')\n",
    "\n",
    "def audio_to_frames(x, n_frame, n_step=None):    \n",
    "\n",
    "    if n_step is None:\n",
    "        n_step = n_frame\n",
    "\n",
    "    if len(x.shape) == 1:\n",
    "        x.shape = (-1,1)\n",
    "\n",
    "    n_overlap = n_frame - n_step\n",
    "    n_frames = (x.shape[0] - n_overlap) // n_step       \n",
    "    n_keep = n_frames * n_step + n_overlap\n",
    "\n",
    "    strides = list(x.strides)\n",
    "    strides[0] = strides[1] * n_step\n",
    "\n",
    "    return np.lib.stride_tricks.as_strided(x[0:n_keep,:], (n_frames,n_frame), strides)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('../full_data/dataset.json') as f:\n",
    "    data = json.load(f)\n",
    "files = data.keys()\n",
    "paths = ['../full_data/real_audios/' + f for f in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y , s = audio_from_file('/media/sa47/Study/PHD/output.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "y = librosa.resample(y, s, 48000, res_type='kaiser_fast')\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.zeros((3,4),dtype=float)\n",
    "b = np.ones((3,4),dtype=float)\n",
    "c = np.hstack((a,b))\n",
    "\n",
    "np.reshape(c,(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(l, n):\n",
    "    res = []\n",
    "    for i in range(0, len(l), n):\n",
    "        res.append(l[i:i+n])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import samplerate\n",
    "# from scipy.io import wavfile\n",
    "# sr, x = wave.read('/media/sa47/Study/PHD/output.wav')  # 48 khz file\n",
    "y_z = samplerate.resample(y, 44100 * 1.0 / 16000, 'sinc_best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['/media/sa47/Study/PHD/output.wav']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'models/vad'\n",
    "# files = paths\n",
    "n_batch=256\n",
    "\n",
    "print('load model from {}'.format(path))\n",
    "\n",
    "if os.path.isdir(path):\n",
    "    candidates = glob.glob(os.path.join(path, 'model.ckpt-*.meta'))\n",
    "    if candidates:\n",
    "        candidates.sort()                \n",
    "        checkpoint_path, _ = os.path.splitext(candidates[-1])\n",
    "else:\n",
    "    checkpoint_path = path\n",
    "\n",
    "if not all([os.path.exists(checkpoint_path + x) for x in ['.data-00000-of-00001', '.index', '.meta']]):\n",
    "    print('ERROR: could not load model')\n",
    "    raise FileNotFoundError\n",
    "\n",
    "vocabulary_path = checkpoint_path + '.json'\n",
    "if not os.path.exists(vocabulary_path):\n",
    "    vocabulary_path = os.path.join(os.path.dirname(checkpoint_path), 'vocab.json')\n",
    "if not os.path.exists(vocabulary_path):\n",
    "    print('ERROR: could not load vocabulary')\n",
    "    raise FileNotFoundError\n",
    "\n",
    "with open(vocabulary_path, 'r') as fp:\n",
    "    vocab = json.load(fp)\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "segments = {}\n",
    "\n",
    "#graph.as_default()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    saver = tf.train.import_meta_graph(checkpoint_path + '.meta')\n",
    "\n",
    "    x = graph.get_tensor_by_name(vocab['x'])\n",
    "    y = graph.get_tensor_by_name(vocab['y'])            \n",
    "    init = graph.get_operation_by_name(vocab['init'])\n",
    "    logits = graph.get_tensor_by_name(vocab['logits'])            \n",
    "    ph_n_shuffle = graph.get_tensor_by_name(vocab['n_shuffle'])\n",
    "    ph_n_repeat = graph.get_tensor_by_name(vocab['n_repeat'])\n",
    "    ph_n_batch = graph.get_tensor_by_name(vocab['n_batch'])\n",
    "    sr = vocab['sample_rate']\n",
    "\n",
    "    sess = tf.Session()\n",
    "    # with tf.Session() as sess:\n",
    "\n",
    "    saver.restore(sess, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "with graph.as_default():\n",
    "    for file in tqdm(files):\n",
    "        old_file = file\n",
    "        #convert_to_16k(file,'temp.wav')\n",
    "#         file = old_file\n",
    "        \n",
    "        if os.path.exists(file):\n",
    "            print(time.time() - start)\n",
    "            sound, sr = audio_from_file(file, sr=sr)\n",
    "            sound = librosa.resample(sound, sr , 48000, res_type='zero_order_hold')\n",
    "            \n",
    "            s_count = 0\n",
    "            for x_data in range(0,len(sound),24000):\n",
    "                print(s_count)\n",
    "                s_count += 0.5\n",
    "                select_data = sound[x_data:x_data+24000]\n",
    "                select_data = np.concatenate((select_data,np.zeros(48000 - len(select_data))))\n",
    "                input = audio_to_frames(select_data, x.shape[1])\n",
    "                labels = np.zeros((input.shape[0],), dtype=np.int32)\n",
    "                sess.run(init, feed_dict = { x : input, y : labels, ph_n_shuffle : 1, ph_n_repeat : 1, ph_n_batch : n_batch })                        \n",
    "                count = 0\n",
    "                n_total = input.shape[0]\n",
    "                while True:\n",
    "                    try:\n",
    "                        output = sess.run(logits) \n",
    "                        labels[count:count+output.shape[0]] = np.argmax(output, axis=1)                                \n",
    "                        count += output.shape[0]\n",
    "                        print('{:.2f}%\\r'.format(100 * (count/n_total)), end='', flush=True)\n",
    "                    except tf.errors.OutOfRangeError:                                                                                \n",
    "                        break                                             \n",
    "                noise = input[np.argwhere(labels==0),:].reshape(-1,1)\n",
    "                speech = input[np.argwhere(labels==1),:].reshape(-1,1)\n",
    "                name, ext = os.path.splitext(file)\n",
    "\n",
    "                start_index = -1\n",
    "                segs = []\n",
    "                for idx_ in range(0,len(labels)):\n",
    "\n",
    "                    if labels[idx_] == 1 and start_index == -1:\n",
    "                        start_index = idx_\n",
    "\n",
    "                    if labels[idx_] == 0 or (labels[idx_] == 1 and idx_ == len(labels) - 1):\n",
    "                        if start_index != -1:\n",
    "                            segs.append([start_index,idx_])\n",
    "                        start_index = -1\n",
    "\n",
    "                print(segs)\n",
    "                segments[ntpath.basename(old_file)] = segs\n",
    "\n",
    "        #                     audio_to_file(os.path.join(name + '.speech' + ext), speech, sr)                    \n",
    "        #                     audio_to_file(os.path.join(name + '.noise' + ext), noise, sr)                                        \n",
    "        #                     return labels, x\n",
    "            else:\n",
    "                print('skip [file not found]')\n",
    "            print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAD:\n",
    "    def __init__(self):\n",
    "        path = 'vad/models/vad'\n",
    "        # files = paths\n",
    "        n_batch=256\n",
    "\n",
    "        print('load model from {}'.format(path))\n",
    "\n",
    "        if os.path.isdir(path):\n",
    "            candidates = glob.glob(os.path.join(path, 'model.ckpt-*.meta'))\n",
    "            if candidates:\n",
    "                candidates.sort()                \n",
    "                checkpoint_path, _ = os.path.splitext(candidates[-1])\n",
    "        else:\n",
    "            checkpoint_path = path\n",
    "\n",
    "        if not all([os.path.exists(checkpoint_path + x) for x in ['.data-00000-of-00001', '.index', '.meta']]):\n",
    "            print('ERROR: could not load model')\n",
    "            raise FileNotFoundError\n",
    "\n",
    "        vocabulary_path = checkpoint_path + '.json'\n",
    "        if not os.path.exists(vocabulary_path):\n",
    "            vocabulary_path = os.path.join(os.path.dirname(checkpoint_path), 'vocab.json')\n",
    "        if not os.path.exists(vocabulary_path):\n",
    "            print('ERROR: could not load vocabulary')\n",
    "            raise FileNotFoundError\n",
    "\n",
    "        with open(vocabulary_path, 'r') as fp:\n",
    "            vocab = json.load(fp)\n",
    "\n",
    "        graph = tf.Graph()\n",
    "\n",
    "        segments = {}\n",
    "\n",
    "        #graph.as_default()\n",
    "\n",
    "        with graph.as_default():\n",
    "\n",
    "            saver = tf.train.import_meta_graph(checkpoint_path + '.meta')\n",
    "\n",
    "            x = graph.get_tensor_by_name(vocab['x'])\n",
    "            y = graph.get_tensor_by_name(vocab['y'])            \n",
    "            init = graph.get_operation_by_name(vocab['init'])\n",
    "            logits = graph.get_tensor_by_name(vocab['logits'])            \n",
    "            ph_n_shuffle = graph.get_tensor_by_name(vocab['n_shuffle'])\n",
    "            ph_n_repeat = graph.get_tensor_by_name(vocab['n_repeat'])\n",
    "            ph_n_batch = graph.get_tensor_by_name(vocab['n_batch'])\n",
    "            sr = vocab['sample_rate']\n",
    "\n",
    "            sess = tf.Session()\n",
    "            # with tf.Session() as sess:\n",
    "\n",
    "            saver.restore(sess, checkpoint_path)\n",
    "        \n",
    "        self.graph = graph\n",
    "        self.sess = sess\n",
    "        \n",
    "    def predict(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = VAD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data = json.load(open('full.json'))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "from tqdm import tqdm\n",
    "count = 0\n",
    "\n",
    "for file in tqdm(data):\n",
    "    if file in segments:\n",
    "        print(file)\n",
    "        #audio = AudioSegment.from_wav('/media/sa47/Intertainment/vnlp/speech/full_data/real_audios/' + file)\n",
    "\n",
    "        segs = data[file]\n",
    "        segs = sorted(segs, key = lambda x : x['start'])\n",
    "        segs.insert(0,{'start':0.0,'end':0.0,'speaker':[0]})\n",
    "\n",
    "        cur_max = 0\n",
    "\n",
    "        for i in range(0,len(segs) - 1):\n",
    "            cur_max = max(cur_max,segs[i]['end'])\n",
    "            if segs[i+1]['start'] - cur_max > 0.5:\n",
    "                start_noise = cur_max\n",
    "                end_noise = segs[i+1]['start']\n",
    "                \n",
    "                print(start_noise,end_noise)\n",
    "#                 noise = audio[int(cur_max*1000) : int(segs[i+1]['start']*1000)]\n",
    "#                 noise.export('EEND/noises/noise_' + str(count) + '.wav',format='wav')\n",
    "#                 count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_index = -1\n",
    "segs = []\n",
    "for idx_ in range(0,len(labels)):\n",
    "    \n",
    "    if labels[idx_] == 1 and start_index == -1:\n",
    "        start_index = idx_\n",
    "    \n",
    "    if labels[idx_] == 0 or (labels[idx_] == 1 and idx_ == len(labels) - 1):\n",
    "        if start_index != -1:\n",
    "            segs.append([start_index,idx_])\n",
    "        start_index = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vad import VAD\n",
    "import numpy as np\n",
    "detector = VAD(frame_duration = 0.5, model_path = 'models/vad')\n",
    "SAMPLING_RATE = 44100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "a = open('/home/ubuntu/vad/test_wavs/LJ002-0292.wav','rb')\n",
    "header = a.read(44)\n",
    "\n",
    "start = time.time()\n",
    "# for i in range(0,10):\n",
    "frames = a.read(44100)\n",
    "array_frames = np.frombuffer(frames,dtype=np.int16)\n",
    "array_frames = array_frames.astype(np.float32, order='C') / 32768.0\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "44100 * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile\n",
    "arr , _ = soundfile.read('/home/ubuntu/chatweb/audio_logs/27-21-07.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "AudioSegment.from_wav('/home/ubuntu/chatweb/audio_logs/27-21-07.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "a = numpy.array([1,2,3,4,5,6])\n",
    "np.tile(a, (3, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile\n",
    "a, _ = soundfile.read('/home/ubuntu/chatweb/audio_logs/27-00-29.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(a) / 48000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.zeros(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = numpy.zeros(48000)\n",
    "\n",
    "a = numpy.reshape(a,(-1, 48000))\n",
    "numpy.reshape(a,(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(open('/home/ubuntu/silent.npy','wb'),silent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "convert_cmd = 'ffmpeg -i ' + response_audio + ' ' + response_audio_wav\n",
    "Popen(convert_cmd.split(), stdout=PIPE, stderr=PIPE).wait()\n",
    "\n",
    "convert_cmd = 'ffmpeg -i ' + response_audio + ' -ar 44100 ' + response_audio_mp3\n",
    "Popen(convert_cmd.split(), stdout=PIPE, stderr=PIPE).wait()\n",
    "\n",
    "nchannels, sampwidth, framerate = extract_audio_info(binaryHeader)\n",
    "cur_user['writer'].setnchannels(nchannels)\n",
    "cur_user['writer'].setsampwidth(sampwidth)\n",
    "cur_user['writer'].setframerate(framerate)\n",
    "cur_user['sampling_rate'] = framerate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
